{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:37.367951Z",
     "start_time": "2019-07-15T21:07:37.347254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/brucecottman/Documents/PROJECTS/paso'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "__file__ = !cd .. ;pwd\n",
    "__file__ = __file__[0]\n",
    "__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:37.377086Z",
     "start_time": "2019-07-15T21:07:37.371383Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/brucecottman/Documents/PROJECTS/paso/lessons',\n",
       " '/Users/brucecottman/anaconda3/envs/paso/lib/python37.zip',\n",
       " '/Users/brucecottman/anaconda3/envs/paso/lib/python3.7',\n",
       " '/Users/brucecottman/anaconda3/envs/paso/lib/python3.7/lib-dynload',\n",
       " '',\n",
       " '/Users/brucecottman/.local/lib/python3.7/site-packages',\n",
       " '/Users/brucecottman/anaconda3/envs/paso/lib/python3.7/site-packages',\n",
       " '/Users/brucecottman/anaconda3/envs/paso/lib/python3.7/site-packages/aeosa',\n",
       " '/Users/brucecottman/anaconda3/envs/paso/lib/python3.7/site-packages/IPython/extensions',\n",
       " '/Users/brucecottman/.ipython',\n",
       " '/Users/brucecottman/Documents/PROJECTS/paso']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from random import random\n",
    "sys.path.append(__file__)\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## paso's Offering of Data Cleaners for your Machine or Deep Learning Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning is a subject that is lightly touched in your brick&mortar or on-line classes. However, in your work as a Data Engineer or Data Scientist you will spend a great deal of your time getting ready (pre-processing) your data so that it can be input into your model. Data cleaning is critical to any production service. Can we find a way to compose a ``sklearn pipeline``  to automate some of your data cleaning?\n",
    "\n",
    "**Scikit-Learn pipelines** are composed of steps, each step has to be **Scikit-Learn transformer** or a custom **Scikit-Learn transformer**. The last of the pipeline can be a transformer or an estimator, where a **Scikit-Learn estimator** is a compatible model. \n",
    "\n",
    "**paso** has quite a few data cleaning transformers that are custom **Scikit-Learn transformers**. Note that **paso** translates from Spanish to English to the word step.\n",
    "\n",
    "**paso** is a package written in Python and some ``C``(for speed) that was originally intended to bundle best-practices and state-of-the-art services, classes and functions for the Machine Learning and Deep Learning community. **paso** has grown beyond this to offer patterns, classes and methods you can use in your **Scikit-Learn pipelines** or custom code with or without adopting the entire **paso** package.\n",
    "\n",
    "The **paso** package consists of a growing set of **paso** services that you can turn on and off for any of your Python projects. Also, included are new state-of-art classes and methods not yet available in **Scikit-Learn**, but are compatible with all of **Scikit-Learn**.\n",
    "\n",
    "**paso** will supply many but not all the tools you need to clean the data. Just because **paso** supplies a tool, does not mean you should use it on your dataset. For example, usually removing rows that have a high density of ``NAs``will result in a worse loss metric or unwanted bias, but not all the time. Iteration of over different cleaning strategies (pipelines) is an important goal of **paso**.\n",
    "\n",
    "Discussion will be divided into the following major segments:\n",
    "- What data cleaning behaviors are currently available in the **paso** package.\n",
    "- How to add a data cleaning **Scikit-Learn pipeline** to your code.\n",
    "- Overview the source code for the x,y,z,a and e class. You are free to use and modify this code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in [lesson-1](), we need to startup **paso** services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:38.816283Z",
     "start_time": "2019-07-15T21:07:37.379572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paso 15.7.2019 17:07:38 INFO Log started\n",
      "paso 15.7.2019 17:07:38 INFO ========================================\n",
      "paso 15.7.2019 17:07:38 INFO Read in parameter file: ../parameters/lesson.2.yaml\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_summary import DataFrameSummary\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "from paso.base import Paso,Log,PasoError\n",
    "from loguru import logger\n",
    "session = Paso(parameters_filepath='../parameters/lesson.2.yaml').startup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the ``boston``data set into the ``City``dataframe. We will munge City up to show what the **paso** cleaners can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:38.916306Z",
     "start_time": "2019-07-15T21:07:38.819994Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paso 15.7.2019 17:07:38 INFO .. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "City = pd.DataFrame(boston.data, columns = boston.feature_names )\n",
    "City['MEDV'] = boston.target\n",
    "logger.info(boston.DESCR)\n",
    "City.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can have some fun and dirty the data. \n",
    "1. First add in the feature ``asv`` which has all the same values .\n",
    "1. Second in the  feature ``saRRIM`` whose values are the same as ``CRIM``.\n",
    "1. Add in the feature ``vlvZN`` that has very low variance.\n",
    "1. Add in the feature ``hcwINDUS`` which with a very high correlation  with the feature ``INDUS``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:38.993405Z",
     "start_time": "2019-07-15T21:07:38.920083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "      <th>asv</th>\n",
       "      <th>saCRIM</th>\n",
       "      <th>vlvZN</th>\n",
       "      <th>hcwINDUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV   asv   saCRIM  vlvZN  hcwINDUS  \n",
       "0     15.3  396.90   4.98  24.0  99.0  0.00632      0     -2.31  \n",
       "1     17.8  396.90   9.14  21.6  99.0  0.02731      1     -7.07  \n",
       "2     17.8  392.83   4.03  34.7  99.0  0.02729      1     -7.07  \n",
       "3     18.7  394.63   2.94  33.4  99.0  0.03237      1     -2.18  \n",
       "4     18.7  396.90   5.33  36.2  99.0  0.06905      1     -2.18  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "City['asv'] = 99.0\n",
    "City['saCRIM'] = City['CRIM']\n",
    "City['vlvZN'] = 1\n",
    "indi = list(City.columns).index('vlvZN')\n",
    "City.iloc[0:1,indi] = 0\n",
    "City['hcwINDUS'] = - City['INDUS']\n",
    "City.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Value to Missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting and correcting for missing and outlier (good or bad) values is an evolving area of research. We will cover outlier values in the lesson on Scalers. For this lesson we will just focus on missing values.\n",
    "\n",
    "Different values can indicate, a value is missing. For example,\n",
    "- ``999`` could mean \"did not answer\" in some features.\n",
    "- ``NA`` could mean not-applicable for this feature/record.\n",
    "- ``-1`` could mean missing for this feature/record-1could mean missing for this feature/record`.\n",
    "and so on.\n",
    "\n",
    "[This is the pandas tutorial on missing data](http://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.101322Z",
     "start_time": "2019-07-15T21:07:38.995819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "      <th>asv</th>\n",
       "      <th>saCRIM</th>\n",
       "      <th>vlvZN</th>\n",
       "      <th>hcwINDUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   NaN  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   NaN   7.07   NaN  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   NaN   7.07   NaN  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   NaN   2.18   NaN  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   NaN   2.18   NaN  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  asv   saCRIM  vlvZN  hcwINDUS  \n",
       "0     15.3  396.90   4.98  24.0  NaN  0.00632    NaN     -2.31  \n",
       "1     17.8  396.90   9.14  21.6  NaN  0.02731    1.0     -7.07  \n",
       "2     17.8  392.83   4.03  34.7  NaN  0.02729    1.0     -7.07  \n",
       "3     18.7  394.63   2.94  33.4  NaN  0.03237    1.0     -2.18  \n",
       "4     18.7  396.90   5.33  36.2  NaN  0.06905    1.0     -2.18  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from paso.pre.cleaners import Transform_Values_Ratios_to_Missing\n",
    "o = Transform_Values_Ratios_to_Missing()\n",
    "x = o.transform(City,inplace=True,missing_values=[99.0,0.0])\n",
    "x.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## paso Class for Determining  Missing Values Ratios for the Features and rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a sufficiently large ratio of missing values for a feature renders it statistically irrelevant, you can remove this feature from the dataset. Similarly for a row with a large ratio of missing values (an observation ) renders it statistically irrelevant,and you remove this row from the dataset.\n",
    "\n",
    "An extra row (each features missing value ratio) and an extra feature (each row missing value ratio) is added to the returned **pandas** dataframe. The missing value ratios are kept in the class instance attribute  ``missing_value_ratios``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.326056Z",
     "start_time": "2019-07-15T21:07:39.103336Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "      <th>asv</th>\n",
       "      <th>saCRIM</th>\n",
       "      <th>vlvZN</th>\n",
       "      <th>hcwINDUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   NaN  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   NaN   7.07   NaN  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   NaN   7.07   NaN  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   NaN   2.18   NaN  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   NaN   2.18   NaN  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  asv   saCRIM  vlvZN  hcwINDUS  \n",
       "0     15.3  396.90   4.98  24.0  NaN  0.00632    NaN     -2.31  \n",
       "1     17.8  396.90   9.14  21.6  NaN  0.02731    1.0     -7.07  \n",
       "2     17.8  392.83   4.03  34.7  NaN  0.02729    1.0     -7.07  \n",
       "3     18.7  394.63   2.94  33.4  NaN  0.03237    1.0     -2.18  \n",
       "4     18.7  396.90   5.33  36.2  NaN  0.06905    1.0     -2.18  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from paso.pre.cleaners import Missing_Values_Ratios\n",
    "o = Missing_Values_Ratios()\n",
    "x = o.transform(City,inplace=True,missing_values=[])\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.377755Z",
     "start_time": "2019-07-15T21:07:39.328133Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM        0.000000\n",
       "ZN          0.735178\n",
       "INDUS       0.000000\n",
       "CHAS        0.930830\n",
       "NOX         0.000000\n",
       "RM          0.000000\n",
       "AGE         0.000000\n",
       "DIS         0.000000\n",
       "RAD         0.000000\n",
       "TAX         0.000000\n",
       "PTRATIO     0.000000\n",
       "B           0.000000\n",
       "LSTAT       0.000000\n",
       "MEDV        0.000000\n",
       "asv         1.000000\n",
       "saCRIM      0.000000\n",
       "vlvZN       0.001976\n",
       "hcwINDUS    0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.features_mvr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.428529Z",
     "start_time": "2019-07-15T21:07:39.380410Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.166667\n",
       "1    0.166667\n",
       "2    0.166667\n",
       "3    0.166667\n",
       "4    0.166667\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.rows_mvr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## paso Class For Imputing a Feature's Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Someone should write a book on filling in missing and bad values of features. Setting these values to their best approximation is key to your data cleaning efforts and further down stream your predictive accuracy and power. But someone did write a [succinct article on the subject:] (https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779).\n",
    "\n",
    "We offer a smorgasbord of impute strategies in *paso's** Impute_Missing_Values. We decided to do by list of features, as impute strategy can vary over subsets of features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T15:15:41.030216Z",
     "start_time": "2019-06-27T15:15:40.979886Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.478419Z",
     "start_time": "2019-07-15T21:07:39.430797Z"
    }
   },
   "outputs": [],
   "source": [
    "#Impute_Features_by_Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## paso Class for Removing Duplicate Features in a DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a feature has the same values by index as another feature then one of those features should be deleted. The duplicate feature is redundant and will have no predictive power. \n",
    "\n",
    "Duplicate features are quite common as an enterprise's database or data lake ages and different data sources are added.\n",
    "\n",
    "The **paso** class to delete duplicate features is given as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.849834Z",
     "start_time": "2019-07-15T21:07:39.480414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paso 15.7.2019 17:07:39 ERROR Passed dataset, DataFrame, contained NA\n"
     ]
    },
    {
     "ename": "PasoError",
     "evalue": "Passed dataset, DataFrame, contained NA",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPasoError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/PROJECTS/paso\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpaso\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleaners\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDupilicate_Features_by_Values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDupilicate_Features_by_Values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/PROJECTS/paso/paso/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m                         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXarg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                     \u001b[0m_Check_No_NA_Values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m                     \u001b[0;31m# pre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/PROJECTS/paso/paso/base.py\u001b[0m in \u001b[0;36m_Check_No_NA_Values\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_Check_No_NA_Values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_Check_No_NA_F_Values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/PROJECTS/paso/paso/base.py\u001b[0m in \u001b[0;36m_Check_No_NA_F_Values\u001b[0;34m(df, feature)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mraise_PasoError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Passed dataset, DataFrame, contained NA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/PROJECTS/paso/paso/base.py\u001b[0m in \u001b[0;36mraise_PasoError\u001b[0;34m(msg)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_PasoError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mPasoError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_isAttribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattribute_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPasoError\u001b[0m: Passed dataset, DataFrame, contained NA"
     ]
    }
   ],
   "source": [
    "from paso.pre.cleaners import Dupilicate_Features_by_Values\n",
    "x = Dupilicate_Features_by_Values().transform(City,inplace=True)\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## paso Class for Removing Zero Variance Features from a DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This class finds all the features which have only one unique value. \n",
    "The variation between values is zero. All these features are removed from\n",
    "the dataset as they have no predictive ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.854573Z",
     "start_time": "2019-07-15T21:07:37.378Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from paso.pre.cleaners import Features_with_Single_Unique_Value\n",
    "x = Features_with_Single_Unique_Value().transform(City,inplace=True)\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.856323Z",
     "start_time": "2019-07-15T21:07:37.380Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "City.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## paso Class for the Determining Variance of Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class finds all the variance of each feature and returns a dataframe where the 1st column is the feature string and the 2nd column is the variance of that feature.This class is a diagnostic tool that is used to decide if the variance is small and thus will have low predictive power.  Care should be used before eliminating any feature and a 2nd opinion of the **SHAP** value should be used in order to reach a decision to remove a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.858077Z",
     "start_time": "2019-07-15T21:07:37.383Z"
    }
   },
   "outputs": [],
   "source": [
    "from paso.pre.cleaners import Features_Variances\n",
    "Features_Variances().transform(City)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## paso Class for the Determining Feature-Feature Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Read this for a good overview of correlation](https://medium.com/fintechexplained/did-you-know-the-importance-of-finding-correlations-in-data-science-1fa3943debc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If any given Feature has an absolute high correlation coefficient with another feature (open interval -1.0,1.0) then is very likely the second one of them will have low predictive power as it is redundant with the other.\n",
    "\n",
    "Usually the Pearson correlation coefficient is used, which is sensitive only to a linear relationship between two variables (which may be present even when one variable is a nonlinear function of the other). A Pearson correlation coefficient of -0.97 is a strong negative correlation while a correlation of 0.10 would be a weak positive correlation.\n",
    "\n",
    "Spearman's rank correlation coefficient is a measure of how well the relationship between two variables can be described by a monotonic function. Kendall's rank correlation coefficient is statistic used to measure the ordinal association between two measured quantities. Spearman's rank correlation coefficient is the more widely used rank correlation coefficient. However, Kendall's is easier to understand.\n",
    "\n",
    "In most of the situations, the interpretations of Kendall and Spearman rank correlation coefficient are very similar to Pearson correlation coefficient and thus usually lead to the same diagnosis. The paso class calculates Peason's,or Spearman's, or Kendall's correlation co-efficients for all feature pairs of the dataset.\n",
    "\n",
    "One of the features of the feature-pair should be removed for its negligible effect on the prediction. Again, this class is a diagnostic that indicates if one feature of will have low predictive power. Care should be used before eliminating any feature to look at the **SHAP** value, (sd/mean) and correlation co-efficient in order to reach a decision to remove a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.859576Z",
     "start_time": "2019-07-15T21:07:37.386Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from paso.pre.cleaners import Feature_Feature_Correlation\n",
    "o = Feature_Feature_Correlation()\n",
    "corr = o.transform(City,threshold= 0.75,inplace=True)\n",
    "\n",
    "corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.861146Z",
     "start_time": "2019-07-15T21:07:37.388Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "o.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## paso Class for the Removing Features from a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I should mention at this point that **sklearn** has a few different algorithms for [feature selection](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#examples-using-sklearn-feature-selection-selectkbest). You might to want to look at these before removing any features.\n",
    "\n",
    "However, I find the feature diagnosis tools given in **paso**, especially **SHAP**.  **SHAP** is state-of-the-art in determining a feature's importance in a model as of 2018 and as far as I know still the state-of-the-art. Please, let us know if you know something better and we add it to **paso**. This is very likely as the field is growing rapidly.\n",
    "\n",
    "Based on your analysis, you are now ready to remove features from a **pandas** dataframe. We have written a wraper class around drop so as to be compatiable with the ``,transform`` method and to include **paso**'s services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T17:25:28.062695Z",
     "start_time": "2019-06-21T17:25:28.045072Z"
    },
    "hidden": true
   },
   "source": [
    "City.drop(['asv','saCRIM','vlvZN','hcwINDUS'],axis=1,inplace=True)\n",
    "City.columnsdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.862627Z",
     "start_time": "2019-07-15T21:07:37.391Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "City.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.864086Z",
     "start_time": "2019-07-15T21:07:37.393Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from paso.pre.cleaners import Remove_Features\n",
    "x = Remove_Features().transform(City,inplace=True,remove=['vlvZN','hcwINDUS']) \n",
    "\n",
    "x.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## paso Class for Removing Features that are not Common to Train and Test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If the train or test datasets have features the other does not, then those features will have no predictive power and should be removed from both datasets. The exception being the target feature that is present in the training dataset of a supervised problem. \n",
    "\n",
    "Features in one dataset (train, test) and not in the other (train, test) may point to other problems in the population of these datasets. You should check the steps in your data loading.\n",
    "\n",
    "Where I have seen this most often is the assembly test dataset from upstream services (Kafka, Google PubSub, Amazon Kinesis Stream, PySpark, and RabbitMQ, to name a few). The features of the test dataset are changed while the pre-trained model (and thus the training set) do not have these new features feature. Depending on your error handling, what happens usually is failure.\n",
    "\n",
    "Using `` ``:\n",
    "1. Differences in the features of the train and test datasets are removed.\n",
    "2. What features have been removed are logged for later reconciliation.\n",
    "3. The prediction from the input test dataset is successfully handled by the pre-trained model.\n",
    "[A good overview to data streamingcan be found at this link.](https://medium.com/analytics-vidhya/data-streams-and-online-machine-learning-in-python-a382e9e8d06a)\n",
    "\n",
    "To create test and train datasets, we can use 30% of ``City`` as test, leaving 70% of ``City`` as train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.865596Z",
     "start_time": "2019-07-15T21:07:37.400Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test, train_target, test_target = train_test_split( City[City.columns.difference(['CRIM'])]\n",
    "                                              ,  City['MEDV']\n",
    "                                              , test_size=0.3\n",
    "                                              , random_state=88)\n",
    "\n",
    "train.shape,train_target.shape, test.shape,  test_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.867393Z",
     "start_time": "2019-07-15T21:07:37.402Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from paso.pre.cleaners import Features_not_in_train_or_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.869233Z",
     "start_time": "2019-07-15T21:07:37.404Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test['extraF'] = 10\n",
    "x = Features_not_in_train_or_test().transform(train,y=test,inplace=False)\n",
    "x[1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the above example, the returned two dataframes are the transform of ``train`` and ``test``. By using ``inplace=False``, the ``test``dataset is not changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.871077Z",
     "start_time": "2019-07-15T21:07:37.406Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## paso Class for Handling Imbalanced Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All avaiable class balance strategies are shown with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.872628Z",
     "start_time": "2019-07-15T21:07:37.409Z"
    }
   },
   "outputs": [],
   "source": [
    "from paso.pre.cleaners import Class_Balance\n",
    "o = Class_Balance('SMOTE')\n",
    "o.classBalancers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the ``iris``data set,which has claas(categorical) into the ``Flower``dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.874299Z",
     "start_time": "2019-07-15T21:07:37.411Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "Flower = pd.DataFrame(iris.data, columns = iris.feature_names )\n",
    "Flower['TypeOf'] = iris.target\n",
    "#logger.info(iris.DESCR)\n",
    "DataFrameSummary(Flower).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.876420Z",
     "start_time": "2019-07-15T21:07:37.413Z"
    }
   },
   "outputs": [],
   "source": [
    "X = Flower[Flower.columns.difference(['TypeOf'])]\n",
    "y = Flower['TypeOf']\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split original dataset into training set(70\\%) and test set(30\\%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.878340Z",
     "start_time": "2019-07-15T21:07:37.416Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.879927Z",
     "start_time": "2019-07-15T21:07:37.418Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "accAcum = 0\n",
    "for round in range(epochs):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    clf=RandomForestClassifier(n_estimators=100)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    accAcum += metrics.accuracy_score(y_test, y_pred)\n",
    "    \n",
    "print(\"Accuracy:\",accAcum/epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.882074Z",
     "start_time": "2019-07-15T21:07:37.420Z"
    }
   },
   "outputs": [],
   "source": [
    "Flower_0 = Flower[Flower['TypeOf']==0]\n",
    "Flower = Flower.append(Flower_0) # append Flower_0\n",
    "print(Flower_0.shape)\n",
    "DataFrameSummary(Flower).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.883853Z",
     "start_time": "2019-07-15T21:07:37.422Z"
    }
   },
   "outputs": [],
   "source": [
    "X = Flower[Flower.columns.difference(['TypeOf'])]\n",
    "y = Flower['TypeOf']\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.885758Z",
     "start_time": "2019-07-15T21:07:37.424Z"
    }
   },
   "outputs": [],
   "source": [
    "X,y = o.transform(X,y)\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Recreate Flower from X,y with ``TypeOf=[1,2]`` articial data added in to balance. This has added effect that ``TypeOf=0`` is removed.\n",
    "1. Add back in base ``TypeOf=0``\n",
    "3. ``Flower``contines 50 each real data for ``TypeOf=[0,1,2]`` and 40 each of synthetic ``TypeOf=[1,2]``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.887708Z",
     "start_time": "2019-07-15T21:07:37.426Z"
    }
   },
   "outputs": [],
   "source": [
    "Flower = pd.DataFrame(X,columns= Flower[Flower.columns.difference(['TypeOf'])].columns)\n",
    "Flower['TypeOf'] = y\n",
    "Flower = Flower[Flower['TypeOf']>0]\n",
    "Flower = Flower.append(Flower_0) # append Flower_0\n",
    "print(Flower_0.shape)\n",
    "DataFrameSummary(Flower).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now balance classes again with oversampler ``SMOOT``. Just ``TypeOf=0`` needs 50 rows of synthetic data to balance the classes. The result is similar to image augumention in that we doubled the ``Isis``dataset with synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.889375Z",
     "start_time": "2019-07-15T21:07:37.428Z"
    }
   },
   "outputs": [],
   "source": [
    "X = Flower[Flower.columns.difference(['TypeOf'])]\n",
    "y = Flower['TypeOf']\n",
    "X,y = o.transform(X,y)\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.890927Z",
     "start_time": "2019-07-15T21:07:37.430Z"
    }
   },
   "outputs": [],
   "source": [
    "Flower = pd.DataFrame(X,columns= Flower[Flower.columns.difference(['TypeOf'])].columns)\n",
    "Flower['TypeOf'] = y\n",
    "# append Flower_0\n",
    "print(Flower_0.shape)\n",
    "DataFrameSummary(Flower).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.892690Z",
     "start_time": "2019-07-15T21:07:37.432Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "accAcum = 0\n",
    "for round in range(epochs):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    clf=RandomForestClassifier(n_estimators=100)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    accAcum += metrics.accuracy_score(y_test, y_pred)\n",
    "    \n",
    "print(\"Accuracy:\",accAcum/epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T19:27:00.376150Z",
     "start_time": "2019-06-28T19:27:00.323431Z"
    }
   },
   "source": [
    " Let us check to see if training is better or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Creating a Pipeline for your Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In a production, data cleaning routines, sequences of transformations that convert raw data into a format useful for analysis, have been viewed as static components that fit into data integration or Extract-Transform-Load (ETL) pipelines and are executed on new data entering the prediction model.\n",
    "\n",
    "However, this perspective fails to take into account that data cleaning is frequently an iterative analysis process for training model or models. paso is constantly involving to support both dynamic production and iterative development data cleaning pipelines.\n",
    "\n",
    "[A good introduction to Scikit-learn pipelines](https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "data_clean_transformer = Pipeline(steps=[\n",
    "    ('Dupilicate_Features_by_Values', Dupilicate_Features_by_Values),\n",
    "    ('Features_with_Single_Unique_Value', Features_with_Single_Unique_Value())\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can display a direct-acyclic-graph(DAG) of your **paso** pipeline."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T21:07:39.894083Z",
     "start_time": "2019-07-15T21:07:37.580Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "session.display_DAG()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary paso data cleaning transformation classes are:\n",
    "- ``Impute_Features_by_Values``\n",
    "- ``Duplicate_Features_by_Values``\n",
    "- ``Features_with_Single_Unique_Value``\n",
    "- `` ``\n",
    "- `` ``\n",
    "- ``Remove_Features``\n",
    "- ``Features_not_in_train_or_test``\n",
    "\n",
    "\n",
    "paso data cleaning analysis classes are:\n",
    "- ``Features_missing_value_ratios``\n",
    "- ``Features_Variances``\n",
    "- ``Feature_Feature_Correlation``\n",
    "- ``Features_SHAP_values ``\n",
    "\n",
    "All paso data cleaning transformation classes are similar do skleaarn transformation classes, support sklearn pipeline classes, but differ from sklearn as paso uses a pandas dataframe for the first argument input and output. sklearn uses a numpy array for input and output.\n",
    "\n",
    "You have seen **paso** offers data cleaning classes for both production data engineers and research data scientists. **paso** support streaming data as well as bulk extraction data cleaning. You can expect **paso** to continue to offer state-of-the-art tools for data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other lessons on **paso** are:\n",
    "1. [**paso**'s Offering of Logging and Parameter Services for your Python Project](https://github.com/bcottman/paso/blob/master/lessons/lesson_1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future, we will cover **paso** in more depth with the following lesons:\n",
    "- Overview of **paso** scalers and handling data outliers.\n",
    "- Overview of **paso** encoders.\n",
    "- Overview of **paso** machine learning and deep learning models.\n",
    "- Using  **paso** on GPUs.\n",
    "- and yet more…\n",
    "\n",
    "If you have a service or feature or see a bug, then leave the **paso** project a [note](https://github.com/bcottman/paso/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
