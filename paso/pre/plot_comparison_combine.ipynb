{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Comparison of the combination of over- and under-sampling algorithms\n\n\nThis example shows the effect of applying an under-sampling algorithms after\nSMOTE over-sampling. In the literature, Tomek's link and edited nearest\nneighbours are the two methods which have been used and are available in\nimbalanced-learn.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>\n# License: MIT\n\nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.svm import LinearSVC\n\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.combine import SMOTEENN, SMOTETomek\n\nprint(__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following function will be used to create toy dataset. It using the\n``make_classification`` from scikit-learn but fixing some parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_dataset(n_samples=1000, weights=(0.01, 0.01, 0.98), n_classes=3,\n                   class_sep=0.8, n_clusters=1):\n    return make_classification(n_samples=n_samples, n_features=2,\n                               n_informative=2, n_redundant=0, n_repeated=0,\n                               n_classes=n_classes,\n                               n_clusters_per_class=n_clusters,\n                               weights=list(weights),\n                               class_sep=class_sep, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following function will be used to plot the sample space after resampling\nto illustrate the characteristic of an algorithm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_resampling(X, y, sampling, ax):\n    X_res, y_res = sampling.fit_resample(X, y)\n    ax.scatter(X_res[:, 0], X_res[:, 1], c=y_res, alpha=0.8, edgecolor='k')\n    # make nice plotting\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.get_xaxis().tick_bottom()\n    ax.get_yaxis().tick_left()\n    ax.spines['left'].set_position(('outward', 10))\n    ax.spines['bottom'].set_position(('outward', 10))\n    return Counter(y_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following function will be used to plot the decision function of a\nclassifier given some data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_decision_function(X, y, clf, ax):\n    plot_step = 0.02\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                         np.arange(y_min, y_max, plot_step))\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    ax.contourf(xx, yy, Z, alpha=0.4)\n    ax.scatter(X[:, 0], X[:, 1], alpha=0.8, c=y, edgecolor='k')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``SMOTE`` allows to generate samples. However, this method of over-sampling\ndoes not have any knowledge regarding the underlying distribution. Therefore,\nsome noisy samples can be generated, e.g. when the different classes cannot\nbe well separated. Hence, it can be beneficial to apply an under-sampling\nalgorithm to clean the noisy samples. Two methods are usually used in the\nliterature: (i) Tomek's link and (ii) edited nearest neighbours cleaning\nmethods. Imbalanced-learn provides two ready-to-use samplers ``SMOTETomek``\nand ``SMOTEENN``. In general, ``SMOTEENN`` cleans more noisy data than\n``SMOTETomek``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2,\n                                                         figsize=(15, 25))\nX, y = create_dataset(n_samples=1000, weights=(0.1, 0.2, 0.7))\n\nax_arr = ((ax1, ax2), (ax3, ax4), (ax5, ax6))\nfor ax, sampler in zip(ax_arr, (\n        SMOTE(random_state=0),\n        SMOTEENN(random_state=0),\n        SMOTETomek(random_state=0))):\n    clf = make_pipeline(sampler, LinearSVC())\n    clf.fit(X, y)\n    plot_decision_function(X, y, clf, ax[0])\n    ax[0].set_title('Decision function for {}'.format(\n        sampler.__class__.__name__))\n    plot_resampling(X, y, sampler, ax[1])\n    ax[1].set_title('Resampling using {}'.format(\n        sampler.__class__.__name__))\nfig.tight_layout()\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}